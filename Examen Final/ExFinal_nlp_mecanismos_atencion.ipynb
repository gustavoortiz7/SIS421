{"cells":[{"cell_type":"markdown","metadata":{"id":"Yg90deR13qYE"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sensioai/blog/blob/master/041_attention/attention.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"4Kzp2jHl3qY7"},"source":["# Mecanismos de Atenci√≥n"]},{"cell_type":"markdown","metadata":{"id":"Uv20V-KqDhH6"},"source":["En el [post](https://sensioai.com/blog/040_encoder_decoder) anterior aprendimos a implementar una arquitectura de red neuronal conocida como `seq2seq`, que utiliza dos redes neuronales (el `encoder` y el `decoder`) para poder trabajar con secuencias de longitud arbitraria tanto a sus entradas como en las salidas. Este modelo nos permite llevar a cabo tareas tales como la traducci√≥n de texto entre dos idiomas, resumir un texto, responder preguntas, etc.\n","\n","![](https://pytorch.org/tutorials/_images/seq2seq.png)\n","\n","Si bien este modelo nos dio buenos resultados, podemos mejorarlo. Si prestamos atenci√≥n a la arquitectura que desarrollamos, el `decoder` (encargado de generar la secuencia de salida) es inicializado con el √∫ltimo estado oculto del `encoder`, el cual tiene la responsabilidad de codificar el significado de toda la frase original. Esto puede ser complicado, sobre todo al trabajar con secuencias muy largas, y para solventar este problema podemos utilizar un mecanismo de `atenci√≥n` que no solo reciba el √∫ltimo estado oculto si no tambi√©n tenga acceso a todas las salidas del `encoder` de manera que el `decoder` sea capaz de \"focalizar su atenci√≥n\" en aquellas partes m√°s importantes. Por ejemplo, para traducir la primera palabra es l√≥gico pensar que lo m√°s importante ser√° la primera palabra y sus adyacentes en la frase original, pero usar el √∫ltimo estado oculto del `encoder` puede no ser suficiente para mantener estas relaciones a largo plazo. Permitir al `decoder` acceder a esta informaci√≥n puede resultar en mejores prestaciones."]},{"cell_type":"markdown","metadata":{"id":"eGFyd0UgDhH7"},"source":["> üí° En la pr√°ctica, los mecanismos de atenci√≥n dan muy buenos resultados en tareas que envuelvan datos secuenciales (como aplicaciones de lenguaje). De hecho, los mejores modelos a d√≠a de hoy para tareas de `NLP` no est√°n basados en redes recurrentes sino en arquitecturas que √∫nicamente implementan mecanismos de atenci√≥n en varias capas. Estas redes neuronales son conocidas como `Transformers`."]},{"cell_type":"markdown","metadata":{"id":"WtdizuVTDhH7"},"source":["## El *dataset*"]},{"cell_type":"markdown","metadata":{"id":"daTZSHr8DhH8"},"source":["Vamos a resolver exactamente el mismo caso que en el post anterior, as√≠ que todo lo que hace referencia al procesado de datos lo dejaremos igual."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G5glMxTtMs_F","outputId":"97da59a8-b1c5-4eac-8bcb-adb5f53d0b3e","executionInfo":{"status":"ok","timestamp":1687524620968,"user_tz":240,"elapsed":20699,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"jjx2B4Q3DhH8","executionInfo":{"status":"ok","timestamp":1687524620968,"user_tz":240,"elapsed":3,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["import unicodedata\n","import re\n","\n","def unicodeToAscii(s):\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn'\n","    )\n","\n","def normalizeString(s):\n","    s = unicodeToAscii(s.lower().strip())\n","    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n","    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n","    return s\n","\n","def read_file(file, reverse=False):\n","    # Read the file and split into lines\n","    lines = open(file, encoding='utf-8').read().strip().split('\\n')\n","\n","    # Split every line into pairs and normalize\n","    pairs = [[normalizeString(s) for s in l.split('\\t')[:2]] for l in lines]\n","\n","    return pairs"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"iuI67myQDhH-","executionInfo":{"status":"ok","timestamp":1687524621288,"user_tz":240,"elapsed":322,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["pairs = read_file('/content/drive/MyDrive/SIS421/EXAMENFINAL/dialogos.txt')\n","#pairs = read_file('spa.txt')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"onkJ27fWDhH-","outputId":"823a987b-7d12-4ea4-edbe-028c1f06b0e8","executionInfo":{"status":"ok","timestamp":1687524621593,"user_tz":240,"elapsed":308,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['hi how are you doing ?', 'i m fine . how about yourself ?']\n","Pregunta: hi how are you doing ?\n","Respuesta: i m fine . how about yourself ?\n"]}],"source":["import random\n","\n","#random.choice(pairs)\n","print(pairs[0] )\n","\n","# Mostrar el primer par de preguntas y respuestas\n","first_pair = pairs[0]\n","pregunt = first_pair[0]\n","respuest = first_pair[1]\n","\n","print(\"Pregunta:\", pregunt)\n","print(\"Respuesta:\", respuest)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"nlP5chOyDhH_","executionInfo":{"status":"ok","timestamp":1687524621594,"user_tz":240,"elapsed":5,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["SOS_token = 0\n","EOS_token = 1\n","PAD_token = 2\n","\n","class Lang:\n","    def __init__(self, language):\n","        self.language = language\n","        self.word2id = {\"SOS\": 0, \"EOS\": 1, \"PAD\": 2}\n","        self.word2count = {}\n","        self.id2word = {0: \"SOS\", 1: \"EOS\", 2: \"PAD\"}\n","        self.n_words = 3  # Count SOS, EOS and PAD\n","\n","    def addQuestion(self, sentence):\n","        for word in sentence.split(' '):\n","            self.addWord(word)\n","\n","    def addWord(self, word):\n","        if word not in self.word2id:\n","            self.word2id[word] = self.n_words\n","            self.word2count[word] = 1\n","            self.id2word[self.n_words] = word\n","            self.n_words += 1\n","        else:\n","            self.word2count[word] += 1\n","\n","    def idsFromQuestion(self, sentence):\n","        return [self.word2id[word] for word in sentence.split(' ')]\n","\n","    def questionFromIds(self, index):\n","        return [self.id2word[ix] for ix in index]"]},{"cell_type":"markdown","metadata":{"id":"GFa0Pg3mDhIA"},"source":["Para poder aplicar la capa de `attention` necesitamos que nuestras frases tengan una longitud m√°xima definida."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bqBjVo-fDhIA","executionInfo":{"status":"ok","timestamp":1687524621594,"user_tz":240,"elapsed":4,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["MAX_LENGTH = 20\n","def filterPairs(pairs, filters, lang=0):\n","    return pairs\n","def trimPairs(pairs):\n","    return [p for p in pairs if len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH]"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0xlL2RpDhIB","outputId":"b8803048-c6d2-485b-99e0-84d9320bfd8b","executionInfo":{"status":"ok","timestamp":1687524621931,"user_tz":240,"elapsed":341,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Tenemos 3725 pares de frases\n","Tenemos 3712 pares de frases con longitud menor de 20\n","Longitud vocabularios:\n","question 2285\n","answer 2339\n"]},{"output_type":"execute_result","data":{"text/plain":["['tomorrow i m going to buy an electric sharpener . EOS',\n"," 'get one with the rubber suction cups on the bottom . EOS']"]},"metadata":{},"execution_count":7}],"source":["def prepareData(file, filters=None, reverse=False):\n","\n","    pairs = read_file(file, reverse)\n","    print(f\"Tenemos {len(pairs)} pares de frases\")\n","\n","    pairs = trimPairs(pairs)\n","    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {MAX_LENGTH}\")\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","        question_lang = Lang('answer')\n","        answer_lang = Lang('question')\n","    else:\n","        question_lang = Lang('question')\n","        answer_lang = Lang('answer')\n","\n","    for pair in pairs:\n","        question_lang.addQuestion(pair[0])\n","        answer_lang.addQuestion(pair[1])\n","\n","        # add <eos> token\n","        pair[0] += \" EOS\"\n","        pair[1] += \" EOS\"\n","\n","    print(\"Longitud vocabularios:\")\n","    print(question_lang.language, question_lang.n_words)\n","    print(answer_lang.language, answer_lang.n_words)\n","\n","    return question_lang, answer_lang, pairs\n","\n","question_lang, answer_lang, pairs = prepareData('/content/drive/MyDrive/SIS421/EXAMENFINAL/dialogos.txt')\n","\n","# descomentar para usar el dataset filtrado\n","#question_lang, answer_lang, pairs = prepareData('spa.txt', filters=eng_prefixes)\n","\n","random.choice(pairs)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4adczw9qDhIB","outputId":"92d6bcef-7b6f-4c3f-ae15-f060e1c25a5b","scrolled":true,"executionInfo":{"status":"ok","timestamp":1687524621932,"user_tz":240,"elapsed":5,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[24, 26, 29, 20, 30, 31, 6]"]},"metadata":{},"execution_count":8}],"source":["answer_lang.idsFromQuestion('what school do you go to .')"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKV0Hk3JDhIC","outputId":"a450c845-9c4e-4135-b2e4-f638a6f46ec9","executionInfo":{"status":"ok","timestamp":1687524621932,"user_tz":240,"elapsed":3,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'wonder', 'poodle', 'fine']"]},"metadata":{},"execution_count":9}],"source":["answer_lang.questionFromIds([3, 1028, 647, 5])"]},{"cell_type":"markdown","metadata":{"id":"2g6d3FnUDhIC"},"source":["En el `Dataset` nos aseguraremos de a√±adir el *padding* necesario para que todas las frases tengan la misma longitud, lo cual no hace necesario utilizar la funci√≥n `collate` que implementamos en el post anterior."]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zi3oPVr7DhIC","outputId":"528ef3b3-bddc-4a1b-b693-1b52ea0bf1e8","executionInfo":{"status":"ok","timestamp":1687524627574,"user_tz":240,"elapsed":5644,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2969, 743)"]},"metadata":{},"execution_count":10}],"source":["import torch\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, question_lang, answer_lang, pairs, max_length):\n","        self.question_lang = question_lang\n","        self.answer_lang = answer_lang\n","        self.pairs = pairs\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, ix):\n","        inputs = torch.tensor(self.question_lang.idsFromQuestion(self.pairs[ix][0]), device=device, dtype=torch.long)\n","        outputs = torch.tensor(self.answer_lang.idsFromQuestion(self.pairs[ix][1]), device=device, dtype=torch.long)\n","        # metemos padding a todas las frases hast a la longitud m√°xima\n","        return torch.nn.functional.pad(inputs, (0, self.max_length - len(inputs)), 'constant', self.question_lang.word2id['PAD']), \\\n","            torch.nn.functional.pad(outputs, (0, self.max_length - len(outputs)), 'constant', self.answer_lang.word2id['PAD'])\n","\n","# separamos datos en train-test\n","train_size = len(pairs) * 80 // 100\n","train = pairs[:train_size]\n","test = pairs[train_size:]\n","\n","dataset = {\n","    'train': Dataset(question_lang, answer_lang, train, max_length=MAX_LENGTH),\n","    'test': Dataset(question_lang, answer_lang, test, max_length=MAX_LENGTH)\n","}\n","\n","len(dataset['train']), len(dataset['test'])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NI-4eXBzDhID","outputId":"b743230f-ed20-458d-c627-9f6cad8ca8bb","executionInfo":{"status":"ok","timestamp":1687524627574,"user_tz":240,"elapsed":6,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([3, 4, 5, 6, 7, 8, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n"," tensor([ 3,  4,  5,  6,  7,  8,  9, 10,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n","          2,  2]))"]},"metadata":{},"execution_count":11}],"source":["question_sentence, answer_sentence = dataset['train'][0]\n","\n","question_sentence, answer_sentence"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bkC8KTkXDhID","outputId":"acb249e5-cf26-4e4f-b0c7-86f7cb7bc3b9","executionInfo":{"status":"ok","timestamp":1687524627575,"user_tz":240,"elapsed":5,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['hi',\n","  'how',\n","  'are',\n","  'you',\n","  'doing',\n","  '?',\n","  'EOS',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD'],\n"," ['i',\n","  'm',\n","  'fine',\n","  '.',\n","  'how',\n","  'about',\n","  'EOS',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD'])"]},"metadata":{},"execution_count":12}],"source":["question_lang.questionFromIds(question_sentence.tolist()), answer_lang.questionFromIds(question_sentence.tolist())"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cSMTf916DhID","outputId":"3707da84-b3b8-4af4-a7fd-a90ab9fc0143","executionInfo":{"status":"ok","timestamp":1687524627575,"user_tz":240,"elapsed":4,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  9,  10, 788,  ...,   2,   2,   2],\n","        [214, 106,  34,  ...,   2,   2,   2],\n","        [369, 123, 244,  ...,   2,   2,   2],\n","        ...,\n","        [255,   6, 167,  ...,   2,   2,   2],\n","        [907, 610, 906,  ...,   2,   2,   2],\n","        [256, 241,  39,  ...,   2,   2,   2]])\n"]}],"source":["dataloader = {\n","    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=64, shuffle=True),\n","    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=128, shuffle=False),\n","}\n","\n","inputs, outputs = next(iter(dataloader['train']))\n","inputs.shape, outputs.shape\n","print(inputs)"]},{"cell_type":"markdown","metadata":{"id":"blqqT_eRDhIE"},"source":["## El modelo"]},{"cell_type":"markdown","metadata":{"id":"dkdKVyp7DhIE"},"source":["En lo que se refiere al `encoder`, seguimos usando exactamente la misma arquitectura. La √∫nica diferencia es que, adem√°s del √∫ltimo estado oculto, necesitaremos todas sus salidas para que el `decoder` pueda usarlas."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"ltJt9863DhIE","executionInfo":{"status":"ok","timestamp":1687524829068,"user_tz":240,"elapsed":283,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["class Encoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=128, hidden_size=128, n_layers=5):\n","        super().__init__()\n","        self.hidden_size = hidden_size\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","\n","    def forward(self, input_sentences):\n","        embedded = self.embedding(input_sentences)\n","        outputs, hidden = self.gru(embedded)\n","        return outputs, hidden\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNp_995_DhIE","outputId":"422b0b35-b1ce-43c6-ac3f-b02111ffba1a","executionInfo":{"status":"ok","timestamp":1687524835879,"user_tz":240,"elapsed":280,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 20, 128])"]},"metadata":{},"execution_count":25}],"source":["encoder = Encoder(input_size=question_lang.n_words)\n","encoder_outputs, encoder_hidden = encoder(torch.randint(0, question_lang.n_words, (64, 20)))\n","\n","# [batch size, seq len, hidden size]\n","encoder_outputs.shape"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJPUWzuuDhIF","outputId":"081253c1-be1c-4d98-f83a-4aa636af8e7e","executionInfo":{"status":"ok","timestamp":1687524838223,"user_tz":240,"elapsed":2,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 64, 128])"]},"metadata":{},"execution_count":26}],"source":["# [num layers, batch size, hidden size]\n","encoder_hidden.shape"]},{"cell_type":"markdown","metadata":{"id":"do1BYr7iDhIF"},"source":["### El *decoder* con *attention*"]},{"cell_type":"markdown","metadata":{"id":"EbSny1MIDhIF"},"source":["Vamos a ver un ejemplo de implementaci√≥n de una capa de atenci√≥n para nuestro `decoder`. En primer lugar tendremos una capa lineal que recibir√° como entradas los `embeddings` y el estado oculto anterior (concatenados). Esta capa lineal nos dar√° a la salida tantos valores como elementos tengamos en nuestras secuencias de entrada (recuerda que las hemos forzado a tener una longitud determinada). Despu√©s, aplicaremos una funci√≥n `softmax` sobre estos valores obteniendo as√≠ una distribuci√≥n de probabilidad que, seguidamente, multiplicaremos por los *outputs* del encoder (que tambi√©n tienen la misma longitud). En esta funci√≥n de probabilidad, cada elemento tiene un valor entre 0 y 1. As√≠ pues, esta operaci√≥n dar√° m√°s importancia a aquellos *outputs* del `encoder` m√°s importantes mientras que al resto les asignar√° unos valores cercanos a 0. A continuaci√≥n, concatenaremos estos valores con los `embeddings`, de nuevo, y se lo daremos a una nueva capa lineal que combinar√° estos `embeddings` con los *outputs* del `encoder` re-escalados para obtener as√≠ los *inputs* finales de la capa recurrente.\n","\n","En resumen, usaremos las entradas y estado oculto del `decoder` para encontrar unos pesos que re-escalar√°n las salidas del `encoder`, los cuales combinaremos de nuevo con las entradas del `decoder` para obtener las representaciones finales de nuestras frases que alimentan la capa recurrente.\n","\n","![](https://i.imgur.com/1152PYf.png)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"CzsdfrcnqlA7","executionInfo":{"status":"ok","timestamp":1687524858685,"user_tz":240,"elapsed":299,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["class DialogDecoder(torch.nn.Module):\n","    def __init__(self, input_size, embedding_size=128, hidden_size=128, n_layers=5, max_length=MAX_LENGTH):\n","        super().__init__()\n","\n","        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n","        self.gru = torch.nn.GRU(embedding_size, hidden_size, num_layers=n_layers, batch_first=True)\n","        self.out = torch.nn.Linear(hidden_size, input_size)\n","\n","        # attention\n","        self.attn = torch.nn.Linear(hidden_size + embedding_size, max_length)\n","        self.attn_combine = torch.nn.Linear(hidden_size * 2, hidden_size)\n","\n","    def forward(self, input_dialogue, hidden, encoder_outputs):\n","        # Obtenemos los embeddings de los turnos de di√°logo\n","        embedded = self.embedding(input_dialogue)\n","\n","        # Calculamos los pesos de atenci√≥n entre los turnos de di√°logo y el estado oculto anterior\n","        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n","        uno=attn_weights.unsqueeze(1)\n","\n","        # Realizamos la atenci√≥n entre los turnos de di√°logo y el encoder_outputs\n","        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n","        # Concatenamos los embeddings con la atenci√≥n aplicada\n","        output = torch.cat((embedded.squeeze(1), attn_applied.squeeze(1)), 1)\n","        output = self.attn_combine(output)\n","        output = torch.nn.functional.relu(output)\n","\n","        # Aplicamos la capa GRU con la entrada ajustada\n","        output, hidden = self.gru(output.unsqueeze(1), hidden)\n","\n","        # Generamos la salida final\n","        output = self.out(output.squeeze(1))\n","\n","        return output, hidden, attn_weights\n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXAw3xHpDhIF","outputId":"8de257e9-572d-4495-e242-be49a5f1e948","executionInfo":{"status":"ok","timestamp":1687524864462,"user_tz":240,"elapsed":325,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-c73859c09972>:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 2339])"]},"metadata":{},"execution_count":28}],"source":["decoder = DialogDecoder(input_size=answer_lang.n_words)\n","decoder_output, decoder_hidden, attn_weights = decoder(torch.randint(0, answer_lang.n_words, (64,1)), encoder_hidden, encoder_outputs)\n","\n","# [batch size, vocab size]\n","decoder_output.shape"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwlFc-8zDhIG","outputId":"cf4e8c69-4c15-43c9-c1c3-c4841474c93e","executionInfo":{"status":"ok","timestamp":1687524871806,"user_tz":240,"elapsed":288,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 64, 128])"]},"metadata":{},"execution_count":29}],"source":["# [num layers, batch size, hidden size]\n","decoder_hidden.shape"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FW_BF-feDhIG","outputId":"142c5414-9130-4245-f2ca-d7b66fd3ebf0","executionInfo":{"status":"ok","timestamp":1687524873906,"user_tz":240,"elapsed":291,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 20])"]},"metadata":{},"execution_count":30}],"source":["# [batch size, max_length]\n","attn_weights.shape"]},{"cell_type":"markdown","metadata":{"id":"XfgvaCeWDhIG"},"source":["## Entrenamiento"]},{"cell_type":"markdown","metadata":{"id":"ypR81P5sDhIG"},"source":["Vamos a implementar el bucle de entrenamiento. En primer lugar, al tener ahora dos redes neuronales, necesitaremos dos optimizadores (uno para el `encoder` y otro para el `decoder`). Al `encoder` le pasaremos la frase en el idioma original, y obtendremos el estado oculto final. Este estado oculto lo usaremos para inicializar el `decoder` que, junto al token `<sos>`, generar√° la primera palabra de la frase traducida. Repetiremos el proceso, utilizando como entrada la anterior salida del decoder, hasta obtener el token `<eos>`."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"QDu8VoLqDhIG","executionInfo":{"status":"ok","timestamp":1687524880728,"user_tz":240,"elapsed":2,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["from tqdm import tqdm\n","import numpy as np\n","\n","def fit(encoder, decoder, dataloader, epochs=10):\n","    encoder.to(device)\n","    decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n","    #criterion = torch.nn.functional.cross_entropy()\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(1, epochs+1):\n","        encoder.train()\n","        decoder.train()\n","        train_loss = []\n","        bar = tqdm(dataloader['train'])\n","        for batch in bar:\n","            input_sentences, output_sentences = batch\n","            bs = input_sentences.shape[0]\n","            loss = 0\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","            # obtenemos el √∫ltimo estado oculto del encoder\n","            encoder_outputs, hidden = encoder(input_sentences)\n","            # calculamos las salidas del decoder de manera recurrente\n","            decoder_input = torch.tensor([[answer_lang.word2id['SOS']] for b in range(bs)], device=device)\n","            for i in range(output_sentences.shape[1]):\n","                output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","                loss += criterion(output, output_sentences[:, i].view(bs))\n","                # el siguiente input ser√° la palabra predicha\n","                decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","            # optimizaci√≥n\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            train_loss.append(loss.item())\n","            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n","\n","        val_loss = []\n","        encoder.eval()\n","        decoder.eval()\n","        with torch.no_grad():\n","            bar = tqdm(dataloader['test'])\n","            for batch in bar:\n","                input_sentences, output_sentences = batch\n","                bs = input_sentences.shape[0]\n","                loss = 0\n","                # obtenemos el √∫ltimo estado oculto del encoder\n","                encoder_outputs, hidden = encoder(input_sentences)\n","                # calculamos las salidas del decoder de manera recurrente\n","                decoder_input = torch.tensor([[answer_lang.word2id['SOS']] for b in range(bs)], device=device)\n","                for i in range(output_sentences.shape[1]):\n","                    output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","                    loss += criterion(output, output_sentences[:, i].view(bs))\n","                    # el siguiente input ser√° la palabra predicha\n","                    decoder_input = torch.argmax(output, axis=1).view(bs, 1)\n","                val_loss.append(loss.item())\n","                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bwNeQeLGDhIH","outputId":"a58ba236-f02f-4dad-c3d7-3aee512820a2","executionInfo":{"status":"ok","timestamp":1687527069988,"user_tz":240,"elapsed":2182534,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/47 [00:00<?, ?it/s]<ipython-input-27-c73859c09972>:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n","Epoch 1/100 loss 72.75503: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 1/100 val_loss 56.95077: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.62it/s]\n","Epoch 2/100 loss 51.25962: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.19it/s]\n","Epoch 2/100 val_loss 56.31011: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.47it/s]\n","Epoch 3/100 loss 50.42215: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 3/100 val_loss 56.56576: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.91it/s]\n","Epoch 4/100 loss 50.18298: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 4/100 val_loss 56.39416: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.59it/s]\n","Epoch 5/100 loss 49.80697: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 5/100 val_loss 56.50121: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.40it/s]\n","Epoch 6/100 loss 49.75793: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 6/100 val_loss 56.47331: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.61it/s]\n","Epoch 7/100 loss 49.50781: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 7/100 val_loss 56.50900: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.56it/s]\n","Epoch 8/100 loss 49.28317: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 8/100 val_loss 57.54175: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.63it/s]\n","Epoch 9/100 loss 48.83863: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 9/100 val_loss 56.84376: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.57it/s]\n","Epoch 10/100 loss 48.54048: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 10/100 val_loss 56.86556: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.44it/s]\n","Epoch 11/100 loss 48.07263: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 11/100 val_loss 57.63895: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.46it/s]\n","Epoch 12/100 loss 47.61518: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 12/100 val_loss 58.25691: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.71it/s]\n","Epoch 13/100 loss 47.06949: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.42it/s]\n","Epoch 13/100 val_loss 58.20851: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.70it/s]\n","Epoch 14/100 loss 46.64078: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 14/100 val_loss 58.96733: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.63it/s]\n","Epoch 15/100 loss 45.91171: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 15/100 val_loss 59.51685: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.56it/s]\n","Epoch 16/100 loss 45.47250: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 16/100 val_loss 60.65226: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.16it/s]\n","Epoch 17/100 loss 45.01544: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.38it/s]\n","Epoch 17/100 val_loss 60.83877: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.51it/s]\n","Epoch 18/100 loss 44.34209: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.20it/s]\n","Epoch 18/100 val_loss 61.24696: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.03it/s]\n","Epoch 19/100 loss 43.92072: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 19/100 val_loss 62.40794: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.43it/s]\n","Epoch 20/100 loss 43.38631: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 20/100 val_loss 62.86028: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.63it/s]\n","Epoch 21/100 loss 43.18735: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.36it/s]\n","Epoch 21/100 val_loss 64.32936: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.41it/s]\n","Epoch 22/100 loss 42.79762: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.20it/s]\n","Epoch 22/100 val_loss 63.82835: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.94it/s]\n","Epoch 23/100 loss 42.47670: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 23/100 val_loss 64.54703: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.43it/s]\n","Epoch 24/100 loss 41.92519: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.16it/s]\n","Epoch 24/100 val_loss 65.42782: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.51it/s]\n","Epoch 25/100 loss 41.72673: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 25/100 val_loss 65.67904: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.62it/s]\n","Epoch 26/100 loss 41.44283: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 26/100 val_loss 65.98315: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.60it/s]\n","Epoch 27/100 loss 40.95409: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 27/100 val_loss 67.46423: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.64it/s]\n","Epoch 28/100 loss 40.72329: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 28/100 val_loss 66.86655: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.84it/s]\n","Epoch 29/100 loss 40.43064: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 29/100 val_loss 67.30797: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.50it/s]\n","Epoch 30/100 loss 40.07644: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.27it/s]\n","Epoch 30/100 val_loss 68.54752: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.38it/s]\n","Epoch 31/100 loss 40.04097: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 31/100 val_loss 68.24750: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.61it/s]\n","Epoch 32/100 loss 39.87515: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 32/100 val_loss 68.35532: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.93it/s]\n","Epoch 33/100 loss 39.40576: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 33/100 val_loss 69.29430: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.68it/s]\n","Epoch 34/100 loss 39.10408: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.34it/s]\n","Epoch 34/100 val_loss 69.13205: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.92it/s]\n","Epoch 35/100 loss 38.93125: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.35it/s]\n","Epoch 35/100 val_loss 69.97747: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.54it/s]\n","Epoch 36/100 loss 38.84900: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.36it/s]\n","Epoch 36/100 val_loss 69.98291: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.96it/s]\n","Epoch 37/100 loss 38.32615: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 37/100 val_loss 71.03928: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.47it/s]\n","Epoch 38/100 loss 38.14255: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.37it/s]\n","Epoch 38/100 val_loss 70.77120: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.92it/s]\n","Epoch 39/100 loss 37.92258: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 39/100 val_loss 72.12962: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.61it/s]\n","Epoch 40/100 loss 37.79421: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 40/100 val_loss 72.21686: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.94it/s]\n","Epoch 41/100 loss 37.23483: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 41/100 val_loss 72.86554: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.59it/s]\n","Epoch 42/100 loss 37.04875: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.43it/s]\n","Epoch 42/100 val_loss 72.99723: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.75it/s]\n","Epoch 43/100 loss 37.07287: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 43/100 val_loss 73.77182: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.50it/s]\n","Epoch 44/100 loss 36.77230: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 44/100 val_loss 73.29040: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.94it/s]\n","Epoch 45/100 loss 36.25938: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 45/100 val_loss 74.22533: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.70it/s]\n","Epoch 46/100 loss 36.41801: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 46/100 val_loss 73.17848: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.45it/s]\n","Epoch 47/100 loss 36.28020: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 47/100 val_loss 74.29923: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.66it/s]\n","Epoch 48/100 loss 35.82878: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 48/100 val_loss 74.76143: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.47it/s]\n","Epoch 49/100 loss 35.43735: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.20it/s]\n","Epoch 49/100 val_loss 75.67918: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.56it/s]\n","Epoch 50/100 loss 35.44599: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 50/100 val_loss 75.32007: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.60it/s]\n","Epoch 51/100 loss 35.08536: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 51/100 val_loss 75.54402: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.53it/s]\n","Epoch 52/100 loss 34.89108: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 52/100 val_loss 76.33123: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.55it/s]\n","Epoch 53/100 loss 34.80078: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.21it/s]\n","Epoch 53/100 val_loss 76.66058: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.52it/s]\n","Epoch 54/100 loss 34.45887: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 54/100 val_loss 76.72695: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.65it/s]\n","Epoch 55/100 loss 34.22327: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.22it/s]\n","Epoch 55/100 val_loss 77.19252: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.48it/s]\n","Epoch 56/100 loss 33.99636: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 56/100 val_loss 76.87700: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.57it/s]\n","Epoch 57/100 loss 33.68745: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 57/100 val_loss 78.65423: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.39it/s]\n","Epoch 58/100 loss 33.74238: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 58/100 val_loss 78.49218: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.60it/s]\n","Epoch 59/100 loss 33.17102: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 59/100 val_loss 78.96305: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.70it/s]\n","Epoch 60/100 loss 32.80316: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 60/100 val_loss 79.23534: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.57it/s]\n","Epoch 61/100 loss 33.07843: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.26it/s]\n","Epoch 61/100 val_loss 79.14654: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.34it/s]\n","Epoch 62/100 loss 32.89981: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.38it/s]\n","Epoch 62/100 val_loss 79.27990: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.67it/s]\n","Epoch 63/100 loss 32.38859: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 63/100 val_loss 80.02166: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  3.00it/s]\n","Epoch 64/100 loss 32.14147: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 64/100 val_loss 80.59349: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.56it/s]\n","Epoch 65/100 loss 32.16228: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.32it/s]\n","Epoch 65/100 val_loss 80.78523: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.87it/s]\n","Epoch 66/100 loss 31.89924: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.36it/s]\n","Epoch 66/100 val_loss 80.28459: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.55it/s]\n","Epoch 67/100 loss 31.57443: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 67/100 val_loss 81.58697: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.91it/s]\n","Epoch 68/100 loss 31.20531: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.34it/s]\n","Epoch 68/100 val_loss 81.81775: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.49it/s]\n","Epoch 69/100 loss 31.01119: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.38it/s]\n","Epoch 69/100 val_loss 81.77797: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.94it/s]\n","Epoch 70/100 loss 30.88769: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n","Epoch 70/100 val_loss 81.78506: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.62it/s]\n","Epoch 71/100 loss 30.69705: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.37it/s]\n","Epoch 71/100 val_loss 81.83088: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.86it/s]\n","Epoch 72/100 loss 30.42833: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 72/100 val_loss 82.60609: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.53it/s]\n","Epoch 73/100 loss 30.35455: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 73/100 val_loss 83.27768: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.11it/s]\n","Epoch 74/100 loss 30.60653: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.28it/s]\n","Epoch 74/100 val_loss 82.85340: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.63it/s]\n","Epoch 75/100 loss 30.00720: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 75/100 val_loss 83.20813: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.47it/s]\n","Epoch 76/100 loss 29.72723: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 76/100 val_loss 83.60924: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.65it/s]\n","Epoch 77/100 loss 29.43925: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.39it/s]\n","Epoch 77/100 val_loss 83.70490: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.64it/s]\n","Epoch 78/100 loss 29.14543: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.24it/s]\n","Epoch 78/100 val_loss 84.79255: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.48it/s]\n","Epoch 79/100 loss 28.83440: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.38it/s]\n","Epoch 79/100 val_loss 85.20164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.73it/s]\n","Epoch 80/100 loss 28.72168: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 80/100 val_loss 85.03103: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.63it/s]\n","Epoch 81/100 loss 28.69496: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.37it/s]\n","Epoch 81/100 val_loss 85.12180: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.66it/s]\n","Epoch 82/100 loss 28.51365: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.21it/s]\n","Epoch 82/100 val_loss 85.48032: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.40it/s]\n","Epoch 83/100 loss 28.24883: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.37it/s]\n","Epoch 83/100 val_loss 85.74735: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.47it/s]\n","Epoch 84/100 loss 28.11935: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.24it/s]\n","Epoch 84/100 val_loss 85.97812: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.37it/s]\n","Epoch 85/100 loss 28.03936: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 85/100 val_loss 85.87501: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.92it/s]\n","Epoch 86/100 loss 27.85382: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.30it/s]\n","Epoch 86/100 val_loss 86.98445: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.55it/s]\n","Epoch 87/100 loss 27.65521: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 87/100 val_loss 86.01956: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.59it/s]\n","Epoch 88/100 loss 27.23835: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.26it/s]\n","Epoch 88/100 val_loss 87.22818: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.54it/s]\n","Epoch 89/100 loss 26.76414: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.40it/s]\n","Epoch 89/100 val_loss 88.25531: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.00it/s]\n","Epoch 90/100 loss 26.46016: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.23it/s]\n","Epoch 90/100 val_loss 88.20157: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.52it/s]\n","Epoch 91/100 loss 26.20030: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.41it/s]\n","Epoch 91/100 val_loss 88.38305: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.74it/s]\n","Epoch 92/100 loss 26.23731: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.21it/s]\n","Epoch 92/100 val_loss 88.98613: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.42it/s]\n","Epoch 93/100 loss 26.08607: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 93/100 val_loss 88.57703: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.35it/s]\n","Epoch 94/100 loss 25.78879: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.24it/s]\n","Epoch 94/100 val_loss 89.10032: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.44it/s]\n","Epoch 95/100 loss 25.59507: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.36it/s]\n","Epoch 95/100 val_loss 88.99345: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  3.12it/s]\n","Epoch 96/100 loss 25.20279: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.24it/s]\n","Epoch 96/100 val_loss 89.65474: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.47it/s]\n","Epoch 97/100 loss 24.92227: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:19<00:00,  2.38it/s]\n","Epoch 97/100 val_loss 90.19644: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.94it/s]\n","Epoch 98/100 loss 24.72896: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.25it/s]\n","Epoch 98/100 val_loss 90.50069: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.38it/s]\n","Epoch 99/100 loss 24.68739: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:21<00:00,  2.21it/s]\n","Epoch 99/100 val_loss 90.82610: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:02<00:00,  2.81it/s]\n","Epoch 100/100 loss 24.45155: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.32it/s]\n","Epoch 100/100 val_loss 90.55884: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.52it/s]\n"]}],"source":["fit(encoder, decoder, dataloader, epochs=100)"]},{"cell_type":"markdown","metadata":{"id":"81pQZ8FODhIH"},"source":["## Generando traducciones"]},{"cell_type":"markdown","metadata":{"id":"zpf0z3bbDhIH"},"source":["Una vez tenemos nuestro modelo entrenado, podemos utilizarlo para traducir frases del ingl√©s al castellano de la siguiente manera."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"zcFY7vzDDhIH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687527138210,"user_tz":240,"elapsed":343,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"db89ae5d-6a62-4984-b02e-15be8ef87eec"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['and',\n","  'people',\n","  'showed',\n","  'up',\n","  '.',\n","  'EOS',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD'],\n"," ['so',\n","  'many',\n","  'people',\n","  'are',\n","  'out',\n","  'of',\n","  'work',\n","  '.',\n","  'EOS',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD',\n","  'PAD'])"]},"metadata":{},"execution_count":33}],"source":["input_sentence, output_sentence = dataset['test'][0]\n","question_lang.questionFromIds(input_sentence.tolist()), answer_lang.questionFromIds(output_sentence.tolist())"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"RzxhNLU8DhIH","executionInfo":{"status":"ok","timestamp":1687527143665,"user_tz":240,"elapsed":328,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["def predict(input_sentence):\n","    # obtenemos el √∫ltimo estado oculto del encoder\n","    encoder_outputs, hidden = encoder(input_sentence.unsqueeze(0))\n","    # calculamos las salidas del decoder de manera recurrente\n","    decoder_input = torch.tensor([[answer_lang.word2id['SOS']]], device=device)\n","    # iteramos hasta que el decoder nos de el token <eos>\n","    outputs = []\n","    decoder_attentions = torch.zeros(MAX_LENGTH, MAX_LENGTH)\n","    i = 0\n","    while True:\n","        output, hidden, attn_weights = decoder(decoder_input, hidden, encoder_outputs)\n","        decoder_attentions[i] = attn_weights.data\n","        i += 1\n","        decoder_input = torch.argmax(output, axis=1).view(1, 1)\n","        outputs.append(decoder_input.cpu().item())\n","        if decoder_input.item() == answer_lang.word2id['EOS']:\n","            break\n","    return answer_lang.questionFromIds(outputs), decoder_attentions"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"nJWDsiGsDhII","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687527149080,"user_tz":240,"elapsed":316,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"634c893f-810a-47a8-9ff8-9b3a01373fbd"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-c73859c09972>:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded.squeeze(1), hidden[0]), dim=1)))\n"]},{"output_type":"execute_result","data":{"text/plain":["['what', 'are', 'that', '?', 'EOS']"]},"metadata":{},"execution_count":35}],"source":["output_words, attn = predict(input_sentence)\n","output_words"]},{"cell_type":"markdown","metadata":{"id":"FVk45xViDhII"},"source":["## Visualizaci√≥n de atenci√≥n"]},{"cell_type":"markdown","metadata":{"id":"ZIwB1jkSDhII"},"source":["Una de las ventajas que nos da la capa de atenci√≥n es que nos permite visualizar en qu√© partes de los inputs se fija el modelo para generar cada una de las palabras en el output, dando un grado de explicabilidad a nuestro modelo (una propiedad siempre deseada en nuestro modelos de `Machine Learning`)."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"ZnOX1xXEDhII","executionInfo":{"status":"ok","timestamp":1687527156764,"user_tz":240,"elapsed":422,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","def showAttention(input_sentence, output_words, attentions):\n","    lim1, lim2 = input_sentence.index('EOS')+1, output_words.index('EOS')+1\n","    fig = plt.figure(dpi=100)\n","    ax = fig.add_subplot(111)\n","    cax = ax.matshow(attentions[:lim2, :lim1].numpy(), cmap='bone')\n","    fig.colorbar(cax)\n","    # Set up axes\n","    ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n","    ax.set_yticklabels([' '] + output_words)\n","    # Show label at every tick\n","    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","    plt.show()"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"vchBo16ODhII","colab":{"base_uri":"https://localhost:8080/","height":518},"executionInfo":{"status":"ok","timestamp":1687527162987,"user_tz":240,"elapsed":906,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"39610307-d697-4df6-b4ef-96df84797954"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-36-c3618fc79f44>:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n","  ax.set_xticklabels([' '] + input_sentence[:lim1], rotation=90)\n","<ipython-input-36-c3618fc79f44>:12: UserWarning: FixedFormatter should only be used together with FixedLocator\n","  ax.set_yticklabels([' '] + output_words)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["showAttention(question_lang.questionFromIds(input_sentence.tolist()), output_words, attn)"]},{"cell_type":"markdown","metadata":{"id":"X07C6dWtFaiI"},"source":["TRANSFER LEAARNING\n","con el transfomer RoBERTa"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"cn2FyTrkF_CO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687527184233,"user_tz":240,"elapsed":15234,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"6a0fb1cf-fbee-4930-a2a0-b93df493ada8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"cCjtyL3cXuUu","executionInfo":{"status":"ok","timestamp":1687527188692,"user_tz":240,"elapsed":2597,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"colab":{"base_uri":"https://localhost:8080/","height":113,"referenced_widgets":["3d0c0dc597b04bf58a8e0c7f42d3c5e9","1ad9d691f67c4a8cb1448858de18e01a","67863a79096243b49fa857d62bfaacea","ed9cc11bcbe241d58935b3d68df84276","6e70172219c84e95ae21b3f7863f45b0","22264ddd1a9e483190e376d547c843f7","90653f4a15b1444e9008c6771046b5ff","095141f9353442b9a4d498f3c5acd01e","c0b1130d54d044e5a587bb0e9d5ca13e","0c3af6c1392340f58efe4d8f9e2456ae","3f24ffe215714b4cba7bd40851483a70","64d4287b1d9c455fa2e71c1f78e27f7c","27ed51e5856941ca86f3ab5d1baae18d","2dc1c2513ba54f2ea73205e9ab550ba6","1a569899935c4436a20d16d2accd57f3","60770fd7e45b41dda06c834f83592aad","d3193fbeb758443db403593edf879297","a0795c15a76f4b4bbee7e44f721a7b14","14982dbb07c847d99dca3a0a4be061c1","94d8beadeda24929a19a48c0be7f010b","98b27338fc604f26bad8781fc944e85e","c2a291b3f42541bcbcff6504baaab5b0","77f31a72776742c19bb81818c8e36dd4","b6ea4cd37a62442c9286a8627f7004f6","dd97afb6d89d464b93b8fc62a7b01c32","f72253a86a6e488386979b372b2d5e5f","c9d205685c2c466591ab3824c54ba5c4","c936a8815a414e5cb9d6813e2a577215","b8abf5f3af8b4e97b7b3039f4ea9e6c8","bf2c28e420394d29bc98289015a0e38b","9173a2762a6641d19afa388f08bd3777","c87a3b7620224ce9b98c90b85b8cf2ab","50224ecd71e14d4187f3537623d95f44"]},"outputId":"540ff1e4-2b58-4958-ce9e-fec7641456b1"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (‚Ä¶)olve/main/vocab.json: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d0c0dc597b04bf58a8e0c7f42d3c5e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (‚Ä¶)olve/main/merges.txt: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64d4287b1d9c455fa2e71c1f78e27f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (‚Ä¶)lve/main/config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77f31a72776742c19bb81818c8e36dd4"}},"metadata":{}}],"source":["import torch\n","from transformers import RobertaModel, RobertaTokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"JSlcP9rvNJzl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687527191799,"user_tz":240,"elapsed":740,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"3fd645f9-afa1-4ece-ffef-296a7f85394e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tenemos 3725 pares de frases\n","Tenemos 3725 pares de frases con longitud menor de 512\n"]},{"output_type":"execute_result","data":{"text/plain":["['i don t think so . after i finish i ll have enough money to go to college .',\n"," 'that s not a bad idea .']"]},"metadata":{},"execution_count":40}],"source":["import random\n","max_length = tokenizer.model_max_length\n","def filterPairs(pairs, filters, lang=0):\n","    return pairs\n","def trimPairs2(pairs):\n","    return [p for p in pairs if len(p[0].split(' ')) < max_length and len(p[1].split(' ')) < max_length]\n","def prepareData(file, filters=None, reverse=False):\n","    pairs = read_file(file, reverse)\n","    print(f\"Tenemos {len(pairs)} pares de frases\")\n","\n","    pairs = trimPairs2(pairs)\n","    print(f\"Tenemos {len(pairs)} pares de frases con longitud menor de {max_length}\")\n","\n","    # Reverse pairs, make Lang instances\n","    if reverse:\n","        pairs = [list(reversed(p)) for p in pairs]\n","\n","    return pairs\n","\n","pairs2 = prepareData('/content/drive/MyDrive/SIS421/EXAMENFINAL/dialogos.txt')\n","\n","random.choice(pairs2)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"QWsMycyhGPm1","executionInfo":{"status":"ok","timestamp":1687528032969,"user_tz":240,"elapsed":302,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["max_length = 40\n","\n","def collate_fn(batch):\n","    questions, answers = zip(*batch)\n","\n","    # Tokenizar y aplicar relleno a las preguntas\n","    question_inputs = tokenizer(questions, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n","    question_input_ids = question_inputs['input_ids']\n","    question_attention_mask = question_inputs['attention_mask']\n","\n","    # Tokenizar y aplicar relleno a las respuestas\n","    answer_inputs = tokenizer(answers, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n","    answer_input_ids = answer_inputs['input_ids']\n","    answer_attention_mask = answer_inputs['attention_mask']\n","\n","    return (question_input_ids, question_attention_mask), (answer_input_ids, answer_attention_mask)\n","class DialogDataset(torch.utils.data.Dataset):\n","    def __init__(self, pairs):\n","        self.pairs = pairs\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, index):\n","        question = self.pairs[index][0]\n","        answer = self.pairs[index][1]\n","        return question, answer\n","\n","train_size = len(pairs2) * 70 // 100\n","train = pairs2[:train_size]\n","test = pairs2[train_size:]"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"gfh5dt0iGPfQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687528082086,"user_tz":240,"elapsed":1036,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"90e17316-bc91-4fee-9796-5ac82b17f2cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["2607 1118\n","torch.Size([256, 40])\n","torch.Size([256, 40])\n","torch.Size([256, 40])\n","torch.Size([256, 40])\n"]}],"source":["dataset = {\n","    'train': DialogDataset(train),\n","    'test': DialogDataset(test)\n","}\n","print(len(dataset['train']), len(dataset['test']))\n","dataloader = {\n","    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=256, shuffle=True, collate_fn=collate_fn),\n","    'test': torch.utils.data.DataLoader(dataset['test'], batch_size=256, shuffle=False, collate_fn=collate_fn)\n","}\n","\n","inputs, outputs = next(iter(dataloader['train']))\n","question_input_ids, question_attention_mask = inputs\n","answer_input_ids, answer_attention_mask = outputs\n","print(question_input_ids.shape)\n","print(question_attention_mask.shape)\n","print(answer_input_ids.shape)\n","print(answer_attention_mask.shape)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"6kEwBE4NGPb1","executionInfo":{"status":"ok","timestamp":1687528093127,"user_tz":240,"elapsed":323,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":50,"metadata":{"id":"6tiAJmJBv7YS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687528097897,"user_tz":240,"elapsed":2130,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"f7831332-efd7-453d-c44a-3732f14c753f"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaModel(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n","    (position_embeddings): Embedding(514, 768, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 768)\n","    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0-11): 12 x RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=768, out_features=768, bias=True)\n","            (key): Linear(in_features=768, out_features=768, bias=True)\n","            (value): Linear(in_features=768, out_features=768, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=768, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          (intermediate_act_fn): GELUActivation()\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=3072, out_features=768, bias=True)\n","          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (activation): Tanh()\n","  )\n",")"]},"metadata":{},"execution_count":50}],"source":["# Obtener la lista de m√≥dulos del modelo\n","model = RobertaModel.from_pretrained('roberta-base')\n","model"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"qNH_NGNSGPZS","executionInfo":{"status":"ok","timestamp":1687528137731,"user_tz":240,"elapsed":2,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["\n","import torch\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","class Encoder(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.roberta = RobertaModel.from_pretrained('roberta-base')\n","\n","        # Congelar los par√°metros del modelo preentrenado\n","        for name, param in self.roberta.named_parameters():\n","            if name.startswith('roberta'):\n","                param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask):\n","        with torch.no_grad():\n","            outputs = self.roberta(input_ids, attention_mask=attention_mask)\n","            encoded_layers = outputs.last_hidden_state\n","            hidden = outputs.pooler_output\n","            #hidden = outputs.hidden_states[-1]\n","        return encoded_layers, hidden"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"VJ7F2tG-X_dZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687528201434,"user_tz":240,"elapsed":41363,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"89ad3dec-74e7-4abe-810d-2c825f6ce397"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([256, 40, 768])\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([256, 768])"]},"metadata":{},"execution_count":52}],"source":["encoder = Encoder()\n","encoder_outputs, encoder_hidden = encoder(question_input_ids, question_attention_mask)\n","\n","# [batch size, seq len, hidden size]\n","print(encoder_outputs.shape)\n","\n","# [num layers, batch size, hidden size]\n","encoder_hidden.shape\n"]},{"cell_type":"code","source":["class DialogDecoder(torch.nn.Module):\n","    def __init__(self, input_size, hidden_size, max_length):\n","        super().__init__()\n","\n","        self.hidden_size = hidden_size\n","        self.max_length = max_length\n","\n","        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n","        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n","        self.attn = torch.nn.Linear(hidden_size + hidden_size, max_length)\n","        self.attn_combine = torch.nn.Linear(hidden_size + hidden_size, hidden_size)\n","        self.out = torch.nn.Linear(hidden_size, input_size)\n","\n","    def forward(self, input_ids, attention_mask, encoder_hidden_states, encoder_outputs):\n","        batch_size, seq_length = input_ids.size()\n","        #print(attention_mask.shape)\n","\n","        embedded = self.embedding(input_ids)\n","        #print(embedded.shape)\n","        embedded1 = embedded * attention_mask.unsqueeze(-1)\n","        #print(embedded1.shape) # Apply attention mask to embeddings\n","        embedded = embedded1.permute(1, 0, 2)\n","        #print(embedded.shape)  # (seq_length, batch_size, hidden_size)\n","\n","        # Aplicamos la capa GRU\n","        _, hidden = self.gru(embedded)\n","        #print(hidden.shape)\n","\n","        # Obtenemos el √∫ltimo estado oculto de la GRU\n","        hidden = hidden[-1].unsqueeze(0)  # (1, batch_size, hidden_size)\n","        #print(hidden.shape)\n","\n","        # Ajustamos las dimensiones para el c√°lculo de la atenci√≥n\n","        hidden = hidden.permute(1, 0, 2)  # (batch_size, 1, hidden_size)\n","        #print(hidden.shape)\n","        hidden = hidden.expand(-1, seq_length, -1)  # (batch_size, seq_length, hidden_size)\n","        #print(hidden.shape)\n","\n","        # Calculamos los pesos de atenci√≥n\n","        attn_weights = torch.nn.functional.softmax(self.attn(torch.cat((embedded1, hidden), dim=2)), dim=2)\n","        #print(\"el\",attn_weights.shape)\n","        #a=encoder_outputs.permute(1, 2, 0)\n","        #print(a.shape)\n","\n","        # Aplicamos la atenci√≥n a los estados ocultos del codificador\n","        attn_applied = torch.bmm(attn_weights, encoder_outputs)\n","        #print(attn_applied.shape)\n","\n","        # Concatenamos los embeddings con la atenci√≥n aplicada\n","        output = torch.cat((embedded1, attn_applied), dim=2)\n","        output = self.attn_combine(output)\n","\n","        # Aplicamos la capa lineal y la funci√≥n de activaci√≥n\n","        output = torch.relu(output)\n","\n","        # Aplicamos la capa lineal de salida\n","        output = self.out(output.squeeze(0))\n","\n","        return output, hidden, attn_weights\n"],"metadata":{"id":"4W68wFRNIBFI","executionInfo":{"status":"ok","timestamp":1687528274270,"user_tz":240,"elapsed":329,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["decoder = DialogDecoder(input_size=tokenizer.vocab_size, hidden_size=768, max_length=40)\n","decoder_output, decoder_hidden, attention_weights = decoder(answer_input_ids, answer_attention_mask, encoder_hidden, encoder_outputs)\n","\n","# [batch size, seq len, vocab size]\n","print(decoder_output.shape)\n","# [batch size, seq len, max_length]\n","print(attention_weights.shape)\n","print(decoder_hidden.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N71_dhnQIf_L","executionInfo":{"status":"ok","timestamp":1687528298505,"user_tz":240,"elapsed":17878,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}},"outputId":"bb75f554-5305-4f25-80bd-85ac9334807a"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([256, 40, 50265])\n","torch.Size([256, 40, 40])\n","torch.Size([256, 40, 768])\n"]}]},{"cell_type":"code","source":["from tqdm import tqdm\n","import numpy as np\n","torch.autograd.set_detect_anomaly(True)\n","def fit(encoder, decoder, dataloader, epochs=10):\n","    encoder.to(device)\n","    decoder.to(device)\n","    encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=1e-3)\n","    decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3)\n","    criterion = torch.nn.CrossEntropyLoss()\n","\n","    for epoch in range(1, epochs+1):\n","        encoder.train()\n","        decoder.train()\n","        train_loss = []\n","        bar = tqdm(dataloader['train'])\n","\n","        for batch in bar:\n","            batch_inputs, batch_outputs = batch\n","            question_input_ids, question_attention_mask = batch_inputs\n","            answer_input_ids, answer_attention_mask = batch_outputs\n","\n","            bs = question_input_ids.shape[0]\n","            loss = 0\n","\n","            encoder_optimizer.zero_grad()\n","            decoder_optimizer.zero_grad()\n","\n","            # Obtenemos el √∫ltimo estado oculto y las salidas del encoder\n","            encoder_outputs, encoder_hidden = encoder(question_input_ids.to(device), question_attention_mask.to(device))\n","            #print(\"los\",encoder_outputs.shape)\n","\n","            # Calculamos las salidas del decoder de manera recurrente\n","            max_length = 40  # Definir la longitud m√°xima deseada\n","            decoder_input = torch.full((bs, max_length), tokenizer.pad_token_id, device=device)\n","            #decoder_input = torch.tensor([[tokenizer.pad_token_id] for _ in range(bs)], device=device)\n","            #print(\"bla bla\",decoder_input.shape)\n","            decoder_hidden = encoder_hidden  # Usamos el estado oculto del encoder como estado oculto inicial del decoder\n","            decoder_input = decoder_input.detach()\n","            for i in range(answer_input_ids.shape[1]):\n","                decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, answer_attention_mask, decoder_hidden, encoder_outputs)\n","                output = decoder_output[:, 0, :]\n","                #print(outputs.shape)\n","                loss += criterion(output, answer_input_ids[:, i])\n","\n","                # El siguiente input ser√° la palabra predicha\n","                #decoder_input = torch.argmax(output, axis=1).unsqueeze(1)\n","                #decoder_input[:, i] = torch.argmax(output, axis=1)\n","                decoder_input[:, i] = torch.argmax(output, axis=1).detach()\n","                #print(\"mis\",decoder_input.shape)\n","\n","            # Optimizaci√≥n\n","            loss.backward()\n","            encoder_optimizer.step()\n","            decoder_optimizer.step()\n","            train_loss.append(loss.item())\n","            bar.set_description(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f}\")\n","\n","        val_loss = []\n","        encoder.eval()\n","        decoder.eval()\n","\n","        with torch.no_grad():\n","            bar = tqdm(dataloader['test'])\n","\n","            for batch in bar:\n","                inputs, outputs = batch\n","                question_input_ids, question_attention_mask = inputs\n","                answer_input_ids, answer_attention_mask = outputs\n","\n","                bs = question_input_ids.shape[0]\n","                loss = 0\n","\n","                # Obtenemos el √∫ltimo estado oculto y las salidas del encoder\n","                encoder_outputs, encoder_hidden = encoder(question_input_ids.to(device), question_attention_mask.to(device))\n","\n","                # Calculamos las salidas del decoder de manera recurrente\n","                decoder_input = torch.full((bs, max_length), tokenizer.pad_token_id, device=device)\n","                decoder_hidden = encoder_hidden  # Usamos el estado oculto del encoder como estado oculto inicial del decoder\n","                decoder_input = decoder_input.detach()\n","                for i in range(answer_input_ids.shape[1]):\n","                    decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, answer_attention_mask, decoder_hidden, encoder_outputs)\n","                    output = decoder_output[:, 0, :]\n","                    loss += criterion(output, answer_input_ids[:, i])\n","\n","                    # El siguiente input ser√° la palabra predicha\n","                    #decoder_input[:, i] = torch.argmax(output, axis=1)\n","                    decoder_input[:, i] = torch.argmax(output, axis=1).detach()\n","\n","                val_loss.append(loss.item())\n","                bar.set_description(f\"Epoch {epoch}/{epochs} val_loss {np.mean(val_loss):.5f}\")\n"],"metadata":{"id":"vepdRzlOytQO","executionInfo":{"status":"ok","timestamp":1687528333082,"user_tz":240,"elapsed":288,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"execution_count":55,"outputs":[]},{"cell_type":"code","source":["fit(encoder, decoder, dataloader, epochs=3)"],"metadata":{"id":"BSXNnSFy0Mmx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbac8eb4-d9f6-4ed1-ef9e-7fb2b3828a40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/11 [00:00<?, ?it/s]"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU-ypf1KGPS_","executionInfo":{"status":"aborted","timestamp":1687524728953,"user_tz":240,"elapsed":11,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["def predict(encoder, decoder, question_input_ids, question_attention_mask, max_length=40):\n","    encoder.to(device)\n","    decoder.to(device)\n","    encoder.eval()\n","    decoder.eval()\n","\n","    with torch.no_grad():\n","        bs = question_input_ids.shape[0]\n","        decoder_input = torch.full((bs, max_length), tokenizer.pad_token_id, device=device)\n","        encoder_outputs, encoder_hidden = encoder(question_input_ids.to(device), question_attention_mask.to(device))\n","        decoder_hidden = encoder_hidden\n","\n","        outputs = []\n","\n","        for i in range(max_length):\n","            decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_outputs)\n","            output = decoder_output[:, 0, :]\n","            predicted_ids = torch.argmax(output, axis=1)\n","            decoder_input[:, i] = predicted_ids\n","            outputs.append(predicted_ids.unsqueeze(1))\n","\n","        outputs = torch.cat(outputs, dim=1)\n","\n","    return outputs\n"]},{"cell_type":"code","source":["question_input_ids = ...  # Datos de entrada de preguntas\n","question_attention_mask = ...  # M√°scaras de atenci√≥n para los datos de entrada de preguntas\n","\n","predicted_output = predict(encoder, decoder, question_input_ids, question_attention_mask)"],"metadata":{"id":"ohwvcAY91pHy","executionInfo":{"status":"aborted","timestamp":1687524728953,"user_tz":240,"elapsed":11,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqoVccIeX7tX","executionInfo":{"status":"aborted","timestamp":1687524728954,"user_tz":240,"elapsed":12,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":["batch = next(iter(dataloader))\n","\n","# Obtener el valor espec√≠fico de question_input_ids y question_attention_mask\n","question_input_ids = batch[0][0]  # Acceder al primer elemento del primer lote\n","question_attention_mask = batch[1][0]  # Acceder al primer elemento del segundo lote\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7HYwTtVnX17a","executionInfo":{"status":"aborted","timestamp":1687524728954,"user_tz":240,"elapsed":12,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nKel_jstGPAr","executionInfo":{"status":"aborted","timestamp":1687524728954,"user_tz":240,"elapsed":12,"user":{"displayName":"Luis Gustavo Ortiz","userId":"13432368426195691699"}}},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"uP0Bx95ZDhII"},"source":["## Resumen"]},{"cell_type":"markdown","metadata":{"id":"Ozvpo0sfDhII"},"source":["En este post hemos visto como introducir mecanismos de atenci√≥n en nuestra arquitectura `encoder-decoder`, los cuales permiten a nuestra red neuronal focalizarse en partes concretas de los *inputs* a la hora de generar los *outputs*. Esta nueva capa no solo puede mejorar nuestros modelos sino que adem√°s tambi√©n es interpretable, d√°ndonos una idea del razonamiento detr√°s de las predicciones de nuestro modelo. Las redes neuronales con mejores prestaciones a d√≠a de hoy en tareas de `NLP`, los `transformers`, est√°n basados enteramente en este tipo de capas de atenci√≥n."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"233.594px"},"toc_section_display":true,"toc_window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"3d0c0dc597b04bf58a8e0c7f42d3c5e9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ad9d691f67c4a8cb1448858de18e01a","IPY_MODEL_67863a79096243b49fa857d62bfaacea","IPY_MODEL_ed9cc11bcbe241d58935b3d68df84276"],"layout":"IPY_MODEL_6e70172219c84e95ae21b3f7863f45b0"}},"1ad9d691f67c4a8cb1448858de18e01a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22264ddd1a9e483190e376d547c843f7","placeholder":"‚Äã","style":"IPY_MODEL_90653f4a15b1444e9008c6771046b5ff","value":"Downloading (‚Ä¶)olve/main/vocab.json: "}},"67863a79096243b49fa857d62bfaacea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_095141f9353442b9a4d498f3c5acd01e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c0b1130d54d044e5a587bb0e9d5ca13e","value":1}},"ed9cc11bcbe241d58935b3d68df84276":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c3af6c1392340f58efe4d8f9e2456ae","placeholder":"‚Äã","style":"IPY_MODEL_3f24ffe215714b4cba7bd40851483a70","value":" 899k/? [00:00&lt;00:00, 7.23MB/s]"}},"6e70172219c84e95ae21b3f7863f45b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22264ddd1a9e483190e376d547c843f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90653f4a15b1444e9008c6771046b5ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"095141f9353442b9a4d498f3c5acd01e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c0b1130d54d044e5a587bb0e9d5ca13e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c3af6c1392340f58efe4d8f9e2456ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f24ffe215714b4cba7bd40851483a70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64d4287b1d9c455fa2e71c1f78e27f7c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_27ed51e5856941ca86f3ab5d1baae18d","IPY_MODEL_2dc1c2513ba54f2ea73205e9ab550ba6","IPY_MODEL_1a569899935c4436a20d16d2accd57f3"],"layout":"IPY_MODEL_60770fd7e45b41dda06c834f83592aad"}},"27ed51e5856941ca86f3ab5d1baae18d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3193fbeb758443db403593edf879297","placeholder":"‚Äã","style":"IPY_MODEL_a0795c15a76f4b4bbee7e44f721a7b14","value":"Downloading (‚Ä¶)olve/main/merges.txt: "}},"2dc1c2513ba54f2ea73205e9ab550ba6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14982dbb07c847d99dca3a0a4be061c1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_94d8beadeda24929a19a48c0be7f010b","value":1}},"1a569899935c4436a20d16d2accd57f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_98b27338fc604f26bad8781fc944e85e","placeholder":"‚Äã","style":"IPY_MODEL_c2a291b3f42541bcbcff6504baaab5b0","value":" 456k/? [00:00&lt;00:00, 8.34MB/s]"}},"60770fd7e45b41dda06c834f83592aad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3193fbeb758443db403593edf879297":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0795c15a76f4b4bbee7e44f721a7b14":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14982dbb07c847d99dca3a0a4be061c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"94d8beadeda24929a19a48c0be7f010b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"98b27338fc604f26bad8781fc944e85e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a291b3f42541bcbcff6504baaab5b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77f31a72776742c19bb81818c8e36dd4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6ea4cd37a62442c9286a8627f7004f6","IPY_MODEL_dd97afb6d89d464b93b8fc62a7b01c32","IPY_MODEL_f72253a86a6e488386979b372b2d5e5f"],"layout":"IPY_MODEL_c9d205685c2c466591ab3824c54ba5c4"}},"b6ea4cd37a62442c9286a8627f7004f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c936a8815a414e5cb9d6813e2a577215","placeholder":"‚Äã","style":"IPY_MODEL_b8abf5f3af8b4e97b7b3039f4ea9e6c8","value":"Downloading (‚Ä¶)lve/main/config.json: 100%"}},"dd97afb6d89d464b93b8fc62a7b01c32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf2c28e420394d29bc98289015a0e38b","max":481,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9173a2762a6641d19afa388f08bd3777","value":481}},"f72253a86a6e488386979b372b2d5e5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c87a3b7620224ce9b98c90b85b8cf2ab","placeholder":"‚Äã","style":"IPY_MODEL_50224ecd71e14d4187f3537623d95f44","value":" 481/481 [00:00&lt;00:00, 23.5kB/s]"}},"c9d205685c2c466591ab3824c54ba5c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c936a8815a414e5cb9d6813e2a577215":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8abf5f3af8b4e97b7b3039f4ea9e6c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf2c28e420394d29bc98289015a0e38b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9173a2762a6641d19afa388f08bd3777":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c87a3b7620224ce9b98c90b85b8cf2ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50224ecd71e14d4187f3537623d95f44":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}